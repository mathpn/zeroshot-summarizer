{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# device = \"cpu\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "\n",
    "arxiv_data = []\n",
    "archive = zipfile.ZipFile('arxiv_dataset.zip', 'r')\n",
    "for line in archive.open(\"arxiv-metadata-oai-snapshot.json\"):\n",
    "    line = json.loads(line)\n",
    "    arxiv_data.append((line[\"title\"].lower(), line[\"abstract\"].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arxiv_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "arxiv_data = random.sample(arxiv_data, 20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "candidate_labels = [\n",
    "    [\"inquiry\", \"answer\"],\n",
    "    [\"simple\", \"complex\"],\n",
    "    [\"abstract\", \"concrete\"],\n",
    "]\n",
    "from transformers.pipelines.pt_utils import PipelineDataset\n",
    "\n",
    "class ZeroshotDataset:\n",
    "    def __init__(self, labels):\n",
    "      self.data = arxiv_data\n",
    "      self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return self.data[idx][0]\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "loader = DataLoader(arxiv_data, batch_size=256)\n",
    "try:\n",
    "    with open(\"labels.json\", \"r\") as label_json:\n",
    "      labels = json.load(label_json)\n",
    "except Exception:\n",
    "    labels = []\n",
    "    for i, (title, _) in enumerate(loader):\n",
    "        if i % 1 == 0:\n",
    "            print(f\"{i}/{len(loader)} processed\")\n",
    "        ll = []\n",
    "        for candidates in candidate_labels:\n",
    "            with torch.no_grad():\n",
    "                result = classifier(list(title), candidates)\n",
    "                ll.extend(x[\"labels\"][0] for x in result)\n",
    "        labels.append(ll)\n",
    "    with open(\"labels.json\", \"w\") as out:\n",
    "        json.dump(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"labels.json\", \"r\") as label_json:\n",
    "      labels = json.load(label_json)\n",
    "\n",
    "def split(list_a, chunk_size):\n",
    "    for i in range(0, len(list_a), chunk_size):\n",
    "        yield list_a[i:i + chunk_size]\n",
    "\n",
    "chunked_labels = []\n",
    "for batch in labels:\n",
    "    chunks = list(split(batch, len(batch) // 3))\n",
    "    chunks = list(zip(*chunks))\n",
    "    chunked_labels.extend(chunks)\n",
    "\n",
    "labels = chunked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TitleAbstractDataset:\n",
    "    def __init__(self, data, descriptors):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title, abstract = self.data[idx]\n",
    "        descriptors = \", \".join(self.descriptors[idx])\n",
    "        return f\"summarize {descriptors}: {abstract.strip()}\", title\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    encoding = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"longest\",\n",
    "        max_length=512, # XXX\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "    target_encoding = tokenizer(\n",
    "        outputs,\n",
    "        padding=\"longest\",\n",
    "        max_length=512,  # XXX\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(arxiv_data))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda:0\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "print(model)\n",
    "model = model.to(device)\n",
    "\n",
    "dataset = TitleAbstractDataset(arxiv_data, labels)\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "  pbar = tqdm(loader)\n",
    "  pbar.set_description(f\"epoch {epoch + 1}\")\n",
    "  loss_ema = None\n",
    "  for i, (input_ids, attention_mask, _labels) in enumerate(pbar):\n",
    "      if (i + 1) % 4 == 0:\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "      model.train()\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      _labels = _labels.to(device)\n",
    "      out = model(input_ids=input_ids, attention_mask=attention_mask, labels=_labels)\n",
    "      loss = out.loss\n",
    "      loss.backward()\n",
    "      if loss_ema is None:\n",
    "          loss_ema = loss.item()\n",
    "      else:\n",
    "          loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
    "      pbar.set_postfix_str(f\"loss = {loss_ema:.3f}\")\n",
    "  \n",
    "      if i % 500 == 0:\n",
    "        torch.save(model.state_dict(), f\"t5_small_ft_{epoch+1}.pth\")\n",
    "        model.eval()\n",
    "        samples = random.sample(range(len(dataset)), k=min(len(dataset), 10))\n",
    "        for sample in samples:\n",
    "            abstract, title = dataset[sample]\n",
    "            input_ids = tokenizer(abstract, return_tensors=\"pt\").input_ids\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model.generate(input_ids)\n",
    "            print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db47ae5badfead9fd39188fd18ff2ce368bc487d9efee61e4db9417ef6acdbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
